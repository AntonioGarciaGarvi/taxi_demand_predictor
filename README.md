# Ride Demand Predictor

End-to-end ML pipeline for predicting NYC taxi demand using MLOps best practices developed as part of Real-World Machine Learning course by Pau Labarta Bajo.

## Overview

This project builds a production-ready machine learning service to predict hourly taxi demand across NYC locations, helping ride-sharing companies optimize driver distribution and reduce lost revenue.

## Architecture

The system consists of three modular pipelines orchestrated via GitHub Actions and local Makefile commands:

### ðŸ”„ Feature Pipeline
Transforms raw taxi data into time-series features and stores them in Hopsworks Feature Store. 

### ðŸ§  Training Pipeline  
Trains LightGBM models using historical features with Optuna hyperparameter optimization. 

### âš¡ Inference Pipeline
Generates hourly predictions using the latest production model.  

<p align="center">
<img src="https://antoniogarciagarvi.github.io/images/portfolio/taxi_demand_prediction/MLsystem.png" align="center">
</p>

## Tech Stack

- **ML**: LightGBM, Optuna, Scikit-learn
- **MLOps**: Hopsworks (Feature Store), Comet ML (Model Registry)
- **Visualization**: Streamlit, Plotly, PyDeck
- **Infrastructure**: GitHub Actions, Poetry
- **Data**: Pandas, NumPy

## Quick Start

1. **Setup environment**
   ```bash
   make init
   ```

2. **Configure credentials**
   Create `.env` file with Hopsworks and Comet ML keys.

3. **Run pipelines**
   ```bash
   make backfill     # Populate historical data (one-time setup)
   make features     # Generate features
   make training     # Train model  
   make inference    # Generate predictions for the last hour
   ```

4. **Launch dashboards**
   ```bash
   make frontend-app     # Prediction dashboard 
   make monitoring-app   # Model monitoring 
   ```

## Key Features

- **Feature Store**: Decouples feature engineering from model training with 672-hour sliding window features 
- **Model Registry**: Automatic model promotion based on MAE threshold (< 30.0) 
- **Monitoring**: Real-time MAE tracking with drift detection
- **Automation**: GitHub Actions workflows for production deployment

## Project Structure

```
â”œâ”€â”€ data/                  # Data storage  
â”‚   â”œâ”€â”€ raw/              # Raw NYC taxi data  
â”‚   â”œâ”€â”€ transformed/      # Processed time-series data  
â”‚   â””â”€â”€ cache/            # Cached intermediate files  
â”œâ”€â”€ models/               # Trained model artifacts  
â”œâ”€â”€ scripts/              # Pipeline implementations  
â”‚   â”œâ”€â”€ feature_pipeline.py  
â”‚   â”œâ”€â”€ training_pipeline.py  
â”‚   â”œâ”€â”€ inference_pipeline.py  
â”‚   â”œâ”€â”€ backfill_feature_group.py  
â”‚   â””â”€â”€ backfill_inference.py  
â”œâ”€â”€ src/                  # Core modules and APIs  
â”‚   â”œâ”€â”€ config.py  
â”‚   â”œâ”€â”€ data.py  
â”‚   â”œâ”€â”€ feature_store_api.py  
â”‚   â”œâ”€â”€ model_registry_api.py  
â”‚   â”œâ”€â”€ inference.py  
â”‚   â”œâ”€â”€ frontend.py  
â”‚   â””â”€â”€ frontend_monitoring.py  
â”œâ”€â”€ notebooks/            # Development notebooks     
â”œâ”€â”€ .github/workflows/    # CI/CD automation  
â”‚   â”œâ”€â”€ feature_pipeline.yaml  
â”‚   â”œâ”€â”€ training_pipeline.yaml  
â”‚   â””â”€â”€ inference_pipeline.yaml  
â”œâ”€â”€ Makefile              # Local development commands  
â””â”€â”€ pyproject.toml        # Poetry dependencies  
```




